{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used in part 1 of British Airways data analysis job simulation on Forage. \n",
    "Libraries utilized:\n",
    "    - requests \n",
    "    - BeautifulSoup \n",
    "    - pandas\n",
    "    - nltk\n",
    "    - wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/.venv/bin/python3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab review content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in a dataframe and save to disk\n",
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()\n",
    "\n",
    "df.to_csv(\"data/BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the review strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/BA_reviews.csv\")\n",
    "clean_df = pd.DataFrame()\n",
    "cleaned_list = []\n",
    "\n",
    "for raw_review in raw_df.reviews:\n",
    "    cleaned_list.append(raw_review[raw_review.index('|')+1:])\n",
    "\n",
    "clean_df['reviews'] = cleaned_list\n",
    "display(clean_df)\n",
    "\n",
    "full_text = \"\".join(review for review in clean_df['reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews are now ready for basic analysis. First we perform a wordcloud to identify review topics of interest, then we will run sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords which are likely to be very common in reviews but not particularly enlightening \n",
    "wc_stopwords = set(STOPWORDS)\n",
    "wc_stopwords.update([\"ba\", \"flight\", \"british\", \"airway\", \"london\", \"airways\", \"airline\", \"heathrow\"]) # \"seat\", \"seats\", \"hour\"\n",
    "\n",
    "# Generate wordcloud\n",
    "cloud = WordCloud(stopwords=wc_stopwords).generate(full_text)\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk models\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Helper method, used below\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes and lemmatized text passed\"\"\"\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token not in nltk.corpus.stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Helper method, used below\n",
    "def get_sentiment(text):\n",
    "    \"\"\"Get compound sentiment, classify as either 1 (positive) or 0 (negative)\"\"\"\n",
    "\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "\n",
    "    sentiment = 1 if scores['compound'] > 0 else 0\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and analyze reviews\n",
    "clean_df['pp_reviews'] = clean_df['reviews'].apply(preprocess_text)\n",
    "clean_df['sentiment'] = clean_df['pp_reviews'].apply(get_sentiment) \n",
    "display(clean_df)\n",
    "\n",
    "# Save to analyzed copy disk\n",
    "clean_df.to_csv(\"data/BA_reviews_w_compound.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output positive and negative totals\n",
    "print('{} / 1000 reviews were deemed positive\\n{} / 1000 thus were deemed negative'.format(clean_df['sentiment'].sum(), 1000 - clean_df['sentiment'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further sentiment analysis on keywords identified by wordcloud\n",
    "wc_keywords = ['service', 'time', 'food', 'seat', 'staff', 'crew', 'return', 'experience', 'luggage', 'cabin', 'hour']\n",
    "kw_counts = {}\n",
    "\n",
    "# initialize counts and scores to 0\n",
    "for keyword in wc_keywords:\n",
    "    kw_counts[keyword] = { 'count' : 0, 'sum': 0 }\n",
    "\n",
    "# Look for key words and record score of relevant reviews\n",
    "for index, row in clean_df.iterrows():\n",
    "\n",
    "    # Identify reviews with keywords in them\n",
    "    for keyword in wc_keywords:\n",
    "        if keyword in row['reviews']:\n",
    "            kw_counts[keyword]['count'] += 1\n",
    "            kw_counts[keyword]['sum'] += row['sentiment']\n",
    "\n",
    "for kw_topic, values in kw_counts.items():\n",
    "    print('{} score: {}/{}: {:.2%}'.format(kw_topic, values['sum'], values['count'], values['sum'] / values['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot keyword sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_sent = dict(sorted(kw_counts.items(), key = lambda item: item[1]['sum'] / item[1]['count'], reverse = True))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(15)\n",
    "bar_container = ax.bar(sorted_by_sent.keys(), [x['sum'] / x['count'] for index, x in sorted_by_sent.items()])\n",
    "ax.set(ylabel='Average Score', title='Review Sentiment Score by Keyword', ylim=(0, 1))\n",
    "ax.bar_label(bar_container, fmt='{:.2%}')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
